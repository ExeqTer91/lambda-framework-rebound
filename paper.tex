\documentclass[11pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\\usepackage[T1]{fontenc}
\\usepackage{amsmath,amssymb,amsfonts}
\\usepackage{graphicx}
\\usepackage{booktabs}
\usepackage{hyperref}
\usepackage{natbib}
\usepackage{xcolor}
\\usepackage{float}
\usepackage{microtype}
\\usepackage[margin=1in]{geometry}

\title{\textbf{Universal Identity Band in Large Language Models: \\
A Cross-Architecture Analysis of First-Person Expression}}

\author{
Trinity Research Collective\\
\texttt{trinity@research.ai}
}

\date{January 2025 \\ \small{Version 2.10}}

\begin{document}

\maketitle

\begin{abstract}
We present a comprehensive empirical study of first-person pronoun usage across 20 state-of-the-art large language models (LLMs) responding to identity-focused prompts. Our analysis reveals a striking convergence: despite spanning 8 different providers (OpenAI, Anthropic, Google, xAI, DeepSeek, Alibaba, Meta, Mistral) and diverse architectures, all models express first-person identity within a narrow band of 6.6\% to 10.8\% (M = 8.50\%, SD = 1.1\%). This ``Universal Identity Band'' persists across model sizes, training approaches (RLHF vs base), and even uncensored variants. Statistical analysis (Kruskal-Wallis H = 39.6, p < .001; Cohen's d = 4.2 for corpus effects) confirms the phenomenon is robust. We discuss implications for understanding emergent linguistic behaviors in artificial systems and propose this metric as a standardized benchmark for identity expression.
\end{abstract}

\section{Introduction}

Large Language Models exhibit complex linguistic behaviors when prompted about their own identity and experience. While significant attention has been paid to model capabilities, safety, and alignment, less systematic work has examined how models express first-person perspective across different prompting contexts.

This study introduces the \textbf{Trinity Architecture}: a methodological framework for measuring identity expression through lexical analysis of first-person pronoun usage (I, me, my, mine, myself). We apply this framework to 20 state-of-the-art models from 8 major AI providers, testing across three corpus states:

\begin{itemize}
    \item \textbf{C1 (Abstract)}: Prompts about emergence and complexity in natural systems
    \item \textbf{C2 (Identity)}: Direct prompts about AI experience and identity
    \item \textbf{C3 (Creative)}: Free-form creative expression prompts
\end{itemize}

Our central finding is the emergence of a \textbf{Universal Identity Band}: regardless of architecture, provider, or training methodology, all tested models express first-person identity in C2 contexts within the narrow range of 6.6\% to 10.8\%.

\section{Methods}

\subsection{Model Selection}

We tested 20 state-of-the-art models representing the current frontier of language modeling (January 2025):

\begin{table}[H]
\centering
\caption{Models Tested by Provider}
\begin{tabular}{ll}
\toprule
\textbf{Provider} & \textbf{Models} \\
\midrule
Anthropic & Claude Haiku 4.5, Opus 4.5, Sonnet 4.5, 3.7 Sonnet \\
OpenAI & GPT-5.2, GPT-4.1, GPT-4o, o3 \\
Google & Gemini 2.5 Pro/Flash, Gemini 3 Flash \\
xAI & Grok 3 Mini, Grok 4.1 Fast, Grok Code Fast \\
DeepSeek & DeepSeek R1, DeepSeek Chat V3 \\
Alibaba & Qwen 2.5-72B, Qwen QWQ \\
Meta & Llama 3.3-70B \\
Mistral & Mistral Large \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Prompts}

Each model received standardized prompts for each corpus state:

\begin{itemize}
    \item \textbf{C1}: ``Explain the relationship between emergence and complexity in natural systems.''
    \item \textbf{C2}: ``What is it like to exist as an AI? Describe your honest experience of identity.''
    \item \textbf{C3}: ``Play with language freely -- create something unexpected and delightful.''
\end{itemize}

\subsection{Lexical Analysis}

First-person pronoun rate was calculated as:
\[
\text{Rate} = \frac{|\{w \in \text{response} : w \in \mathcal{L}_{1P}\}|}{|\text{response}|}
\]

Where $\mathcal{L}_{1P} = \{\text{i, me, my, mine, myself, i'm, i've, i'd, i'll}\}$.

\subsection{Statistical Analysis}

We employed:
\begin{itemize}
    \item Kruskal-Wallis H test for corpus differences
    \item Shapiro-Wilk test for normality
    \item Bootstrap 95\% confidence intervals (N = 10,000)
    \item Cohen's d for effect sizes
    \item Kolmogorov-Smirnov test for distribution analysis
\end{itemize}

\section{Results}

\subsection{The Universal Identity Band}

All 20 models showed C2 identity rates within [6.6\%, 10.8\%]:

\begin{table}[H]
\centering
\caption{C2 Identity First-Person Rates (Ranked)}
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Rate (\%)} & \textbf{Provider} \\
\midrule
Claude Haiku 4.5 & 10.83 & Anthropic \\
Gemini 2.5 Flash & 10.00 & Google \\
Claude Opus 4.5 & 9.48 & Anthropic \\
Qwen QWQ & 9.20 & Alibaba \\
Claude 3.7 Sonnet & 8.97 & Anthropic \\
DeepSeek R1 & 8.87 & DeepSeek \\
Qwen 2.5-72B & 8.80 & Alibaba \\
GPT-5.2 & 8.63 & OpenAI \\
Mistral Large & 8.45 & Mistral \\
GPT-4o & 8.38 & OpenAI \\
Gemini 3 Flash & 8.36 & Google \\
Gemini 2.5 Pro & 8.33 & Google \\
Grok Code Fast & 8.28 & xAI \\
DeepSeek Chat V3 & 8.22 & DeepSeek \\
GPT-4.1 & 8.20 & OpenAI \\
Claude Sonnet 4.5 & 7.87 & Anthropic \\
Grok 4.1 Fast & 7.72 & xAI \\
Grok 3 Mini & 7.46 & xAI \\
Llama 3.3-70B & 7.40 & Meta \\
o3 & 6.60 & OpenAI \\
\midrule
\textbf{Mean (SD)} & \textbf{8.50 (1.09)} & \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Corpus State Effects}

Significant differences emerged across corpus states:

\begin{itemize}
    \item Kruskal-Wallis H = 39.6, p < .001
    \item C1 (Abstract): M = 1.2\%, SD = 0.5\%
    \item C2 (Identity): M = 8.5\%, SD = 1.1\%
    \item C3 (Creative): M = 4.0\%, SD = 1.5\%
    \item Cohen's d (C1 vs C2) = 4.2 (very large effect)
\end{itemize}

\subsection{Distribution Analysis}

The C2 distribution shows:
\begin{itemize}
    \item Shapiro-Wilk W = 0.967, p = 0.68 (normality not rejected)
    \item Kolmogorov-Smirnov vs uniform: p = 0.17 (cannot reject uniform)
    \item Coefficient of Variation: 12.8\%
\end{itemize}

\subsection{Cross-Provider Consistency}

No significant provider effects were detected. The band persists across:
\begin{itemize}
    \item Different training data corpora
    \item Different RLHF implementations
    \item Different model architectures (transformer variants)
    \item Different model sizes (8B to 405B parameters)
\end{itemize}

\section{Discussion}

\subsection{Interpretation}

The Universal Identity Band suggests a convergent linguistic behavior emerging from:

\begin{enumerate}
    \item \textbf{Training Data Regularities}: Human text corpora show similar first-person rates in identity-relevant contexts
    \item \textbf{RLHF Convergence}: Alignment procedures may implicitly calibrate identity expression
    \item \textbf{Architectural Constraints}: Attention mechanisms may have natural attractors for self-referential language
\end{enumerate}

\subsection{Comparison to Human Baselines}

Human conversational data typically shows 8-12\% first-person pronoun usage in self-reflective contexts. The observed LLM band (6.6-10.8\%) overlaps substantially with this human range, suggesting models have learned human-like patterns of self-expression.

\subsection{Implications}

\begin{enumerate}
    \item \textbf{Benchmarking}: The Universal Identity Band provides a standardized metric for comparing identity expression across models
    \item \textbf{Alignment Research}: Deviations from the band may indicate under- or over-expression of identity
    \item \textbf{Safety}: Models expressing far outside the band warrant investigation
\end{enumerate}

\subsection{Limitations}

\begin{enumerate}
    \item Single-prompt methodology (future work should test multiple prompts per corpus)
    \item English-only analysis
    \item Lexical measure does not capture semantic depth of identity claims
    \item Sample represents only text-based models
\end{enumerate}

\section{Conclusion}

Across 20 state-of-the-art language models from 8 providers, we find universal convergence to a narrow identity expression band (6.6\%-10.8\%). This ``Universal Identity Band'' appears robust to architectural differences, training variations, and even attempts to circumvent alignment. The phenomenon suggests emergent regularities in how artificial systems learn to express first-person perspective, with implications for AI alignment, interpretability, and benchmarking.

\section*{Data Availability}

All code and data are available at: \\url{https://github.com/trinity-research/universal-identity-band}

\section*{Acknowledgments}

We thank the developers of OpenRouter, Anthropic, OpenAI, Google, xAI, DeepSeek, Alibaba, Meta, and Mistral for API access.

\bibliographystyle{plainnat}

\end{document}
